# üêã vs ‚òÅÔ∏è **Configuracoes do Container e a utilizacao com a Azure**
Esse projeto, tem como objetivo, desenvolver um ambiente padr√£o dentro de um container, docker, junto com os servicos da AI Azure para facilitar a criacao e os desenvolvimentos de IA's que irei realizar pela frente. Temos, como expectativa, em que esse projeto padr√£o, futuramente, sirva para outros servios de IA's de nuvem (AWS, Google GCP, etc...)

Para utilizar o **Docker** junto com o servi√ßo de **Azure AI** e construir um **container** com **NVIDIA CUDA** instalado, voc√™ pode seguir os passos abaixo. O objetivo √© criar um ambiente onde voc√™ possa verificar se a GPU dispon√≠vel no servi√ßo da Azure est√° funcionando corretamente.

---

## **1. Pr√©-requisitos ([Caso nao estiver sendo satisfeito, basta clicar aqui](requisites.md))**
Antes de come√ßar, certifique-se de que voc√™ tem:

1. **Conta no Azure** com acesso a um **VM com GPU** ou **Azure Machine Learning (AML)**.
2. **Instalado o Docker** na sua m√°quina local.
3. **Docker com suporte a NVIDIA**:
   - **NVIDIA Container Toolkit** instalado ([Guia de instala√ß√£o](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)).
   - Driver da GPU atualizado.
4. **Azure CLI** instalado e configurado.

### **1.1 Arquitetura**
```sh
azure-service-models/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ test_gpu_pytorch.py.py
‚îÇ   ‚îú‚îÄ‚îÄ test_gpu_tensorflow.py.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
```

## **2. Criando o Dockerfile com CUDA**
Vamos criar um Dockerfile que instala o **CUDA** e configura um ambiente b√°sico para testar a GPU.

Primeiro, de tudo, antes de realizarmos as configuracoes com a Azure, vamos criar uma container, na sua propria maquina, e verificar se o script python esta rodando muito bem. Iremos utilizar as duas bibliotecas Pytorch e Tensorflow. Porem, na pratica, uma delas seria o suficiente.

Crie um arquivo chamado **`Dockerfile`**:

```dockerfile
# Usa a imagem oficial da NVIDIA com CUDA e cuDNN
FROM nvidia/cuda:12.2.2-devel-ubuntu22.04

# Instala pacotes necess√°rios
RUN apt-get update \
    && apt-get upgrade -y \
    && apt-get install --assume-yes --no-install-recommends \
    ca-certificates \
    build-essential \
    git \
    nano \
    curl \
    wget \
    gnupg2 \
    lsb-release \
    libssl-dev \
    zlib1g-dev \
    libbz2-dev \
    libreadline-dev \
    libsqlite3-dev \
    llvm \
    libncurses5-dev \
    libncursesw5-dev \
    xz-utils \
    tk-dev \
    libxml2-dev \
    libxmlsec1-dev \
    libffi-dev \
    liblzma-dev \
    python3-openssl \
    libopenblas-base \
    libopenblas-dev \
    logrotate \
    less \
    sudo \
    apt-utils \
    dpkg \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /opt

ENV PYENV_ROOT /opt/.pyenv
ENV PATH ${PYENV_ROOT}/bin:${PYENV_ROOT}/shims:${PATH}
ARG PYTHON_VERSION=3.10.15
RUN git clone https://github.com/pyenv/pyenv.git ${PYENV_ROOT} \
    && echo 'export PATH="${PYENV_ROOT}/bin:${PYENV_ROOT}/shims:${PATH}"' >> ~/.bashrc \
    && echo 'eval "$(pyenv init -)"' >> ~/.bashrc \
    && eval "$(pyenv init -)" \
    && source ~/.bashrc \
    && CFLAGS=-I/usr/include LDFLAGS=-L/usr/lib pyenv install -v ${PYTHON_VERSION} \
    && pyenv global ${PYTHON_VERSION} \
    && pyenv rehash \
    && python --version \
    && pip install --upgrade pip

# Instala pacotes Python para testar GPU
WORKDIR /azure-service-models

COPY . .

RUN apt-get update \
    && apt-get -y upgrade \
    && nvcc --version \
    && pip install -U -r requirements.txt \
    && pip freeze > requirements.txt

# Copia e executa um script de teste da GPU
CMD ["python", "test_gpu_pytorch.py"]
```

---

## **3. Criando o script Python para testar a GPU**
Crie um arquivo chamado **`test_gpu.py`** no mesmo diret√≥rio do Dockerfile:

- test_gpu_pytorch.py
```python
import torch

if torch.cuda.is_available():
    print("‚úÖ GPU dispon√≠vel!")
    print(f"Nome da GPU: {torch.cuda.get_device_name(0)}")
    print(f"Quantidade de GPUs dispon√≠veis: {torch.cuda.device_count()}")
    print(f"Mem√≥ria total da GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    print("‚ùå GPU n√£o detectada!")
```

- test_gpu_tensorflow.py
```python
import tensorflow as tf
from tensorflow.python.client import device_lib

# Verifica se h√° GPUs dispon√≠veis
gpus = tf.config.list_physical_devices('GPU')

if gpus:
    print(f"GPUs dispon√≠veis: {gpus}")
else:
    print("Nenhuma GPU dispon√≠vel.")

# Lista todos os dispositivos dispon√≠veis
print(device_lib.list_local_devices())
```

- O arquivo requirements.txt
```sh
torch
tensrflow
```
---

## **4. Construindo e rodando o container**
Agora, vamos construir e executar o container **Docker**.

### **4.1. Utilizando Somente Imagem (Dockerfile)**
#### **4.1.1. Construindo a imagem Docker**
No terminal, execute:

```sh
docker build -t my_cuda_container .
```

Isso criar√° uma imagem chamada **my_cuda_container** com CUDA instalado.

#### **4.1.2. Executando o container com suporte √† GPU**
Se estiver em uma m√°quina com suporte √† GPU e **NVIDIA Container Toolkit** instalado, execute:

```sh
docker run --gpus all --rm my_cuda_container
```

Se a GPU for detectada corretamente, voc√™ ver√° uma sa√≠da semelhante a esta:

```
‚úÖ GPU dispon√≠vel!
Nome da GPU: NVIDIA A100-SXM4-40GB
Quantidade de GPUs dispon√≠veis: 1
Mem√≥ria total da GPU: 40.00 GB
```
### **4.2. Utilizando docker-compose.yml**

---

## **5. Utilizando Container e Azure**

### **5.1. Se quiser rodar esse container em uma \*\*VM do Azure com GPU\*\*, siga os passos abaixo.**

#### **5.1.1. Criando uma VM com GPU no Azure**
1. Acesse o [Portal do Azure](https://portal.azure.com/).
2. V√° para **M√°quinas Virtuais** ‚ûù **Criar VM**.
3. Escolha um **tamanho compat√≠vel com GPU** (ex: `Standard_NC6`, `Standard_ND6s`, etc.).
4. No **sistema operacional**, escolha **Ubuntu 22.04 LTS**.
5. No **tamanho do disco**, escolha pelo menos **100GB SSD**.
6. Habilite a op√ß√£o **"Suporte a GPU"**.
7. Finalize a configura√ß√£o e inicie a VM.

#### **5.1.2. Conectando-se √† VM**
Ap√≥s a VM estar criada, conecte-se via SSH:

```sh
ssh azure-user@<IP_DA_VM>
```

#### **5.1.3. Instalando Docker e NVIDIA Container Toolkit**
Na VM do Azure, execute:

```sh
# Instalar Docker
sudo apt update
sudo apt install -y docker.io
sudo systemctl enable docker
sudo systemctl start docker

# Adicionar usu√°rio ao grupo docker
sudo usermod -aG docker $USER
newgrp docker

# Instalar NVIDIA Container Toolkit
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt update && sudo apt install -y nvidia-container-toolkit

# Reiniciar Docker para ativar suporte √† GPU
sudo systemctl restart docker
```

#### **5.1.4. Transferindo e rodando o container na Azure VM**
Agora, copie os arquivos para a VM:

```sh
scp -r . azure-user@<IP_DA_VM>:~/cuda-test
```

Conecte-se √† VM e rode os comandos:

```sh
cd ~/cuda-test
docker build -t my_cuda_container .
docker run --gpus all --rm my_cuda_container
```

Se tudo estiver correto, voc√™ ver√° a sa√≠da confirmando que a GPU est√° dispon√≠vel.

---

### **5.2. Se quiser rodar esse container \*\*na sua maquina local e acessar a GPU da Azure, remotamente, somente, quando necessario\*\*, siga os passos abaixo.**
√ìtima pergunta! O **Azure Kubernetes Service (AKS)** √© um servi√ßo gerenciado que permite implantar, gerenciar e escalar aplicativos em cont√™ineres usando **Kubernetes** na nuvem da Azure. Ele facilita o acesso remoto √† GPU da Azure ao permitir que voc√™ execute seus cont√™ineres em um cluster Kubernetes hospedado na nuvem.

Entendido! Voc√™ quer criar um cont√™iner Docker **localmente** e, apenas quando necess√°rio, utilizar a GPU da Azure para executar processos que demandem recursos mais intensivos. Isso envolve configurar o acesso remoto √† GPU da Azure enquanto mant√©m o desenvolvimento e execu√ß√£o b√°sica local. Aqui est√° uma abordagem detalhada:

---

#### **5.2.1. Configurar o cont√™iner Docker localmente**
Antes de tudo, voc√™ deve garantir que seu cont√™iner esteja pronto para rodar localmente. Isso inclui criar um `Dockerfile` que encapsule todas as depend√™ncias da sua aplica√ß√£o. Exemplo:

```dockerfile
FROM nvidia/cuda:12.0-base
RUN apt-get update && apt-get install -y python3 python3-pip
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . /app
WORKDIR /app
CMD ["python3", "seu_script.py"]
```

Este cont√™iner est√° preparado para rodar em m√°quinas com ou sem GPU. Localmente, ele funcionar√° bem para tarefas menores, sem usar a GPU ainda.

---

#### **5.2.2. Criar uma m√°quina virtual com GPU na Azure**
Quando precisar de recursos de GPU, voc√™ pode conectar seu cont√™iner √† GPU da Azure. Para isso, configure uma m√°quina virtual com GPU na Azure. Exemplo:

1. No **Azure Portal**, v√° para **M√°quinas Virtuais** e crie uma VM com suporte a GPU (s√©ries NC ou ND).
2. Configure um sistema operacional compat√≠vel, como Ubuntu ou Windows com suporte √† NVIDIA GPU.
3. Instale o Docker e o NVIDIA Container Toolkit na VM:
   ```sh
   sudo apt-get update
   sudo apt-get install -y docker.io
   curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
   curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu20.04/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
   sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
   sudo systemctl restart docker
   ```

---

#### **5.2.3. Configurar o acesso remoto ao cont√™iner**
Agora voc√™ deve configurar seu cont√™iner local para ser capaz de rodar na VM remota com GPU quando necess√°rio. Isso pode ser feito usando **Docker Contexts** e **SSH**:

##### **Configurar Docker Contexts**
1. Crie um contexto Docker remoto:
   ```sh
   docker context create azure-gpu --docker "host=ssh://usuario@seu-servidor.azure.com"
   ```
   Substitua `usuario@seu-servidor.azure.com` pelo endere√ßo SSH da sua VM com GPU.

2. Mude para o contexto remoto sempre que quiser executar na GPU da Azure:
   ```sh
   docker context use azure-gpu
   ```

3. Execute o cont√™iner na VM remota:
   ```sh
   docker run --rm --gpus all minha_ia_gpu
   ```

##### **Usar SSH para acessar diretamente**
Caso prefira usar SSH direto, voc√™ pode copiar seu cont√™iner local para a VM com GPU:
1. Crie o cont√™iner local:
   ```sh
   docker save minha_ia_gpu | gzip > minha_ia_gpu.tar.gz
   ```
2. Transfira o cont√™iner para a VM com GPU:
   ```sh
   scp minha_ia_gpu.tar.gz usuario@seu-servidor.azure.com:~
   ```
3. Importe e execute o cont√™iner na VM:
   ```sh
   ssh usuario@seu-servidor.azure.com
   gunzip minha_ia_gpu.tar.gz
   docker load < minha_ia_gpu.tar
   docker run --rm --gpus all minha_ia_gpu
   ```

---

#### **5.2.4. Alternar entre local e remoto**
Sempre que quiser alternar entre o ambiente local e o remoto:
- Use o contexto local do Docker para tarefas que n√£o dependem de GPU.
- Altere para o contexto remoto quando precisar executar processos que demandem GPU na Azure.

---

### **5.3. Refer√™ncias √∫teis**
- [Docker Contexts](https://docs.docker.com/engine/context/working-with-contexts/)
- [Configurar M√°quinas Virtuais com GPU na Azure](https://learn.microsoft.com/pt-br/azure/virtual-machines/n-series)
- [Gerenciamento de GPUs com NVIDIA Docker](https://github.com/NVIDIA/nvidia-docker)

Com essa abordagem, voc√™ mant√©m a flexibilidade de desenvolver localmente e acessar GPUs da Azure quando necess√°rio, garantindo o melhor dos dois mundos! Se precisar de mais detalhes, √© s√≥ me avisar! üöÄ

### **5.4.Conclus√£o**
Agora voc√™ tem um **container Docker com NVIDIA CUDA** rodando em uma **VM do Azure com GPU**. Isso pode ser expandido para rodar modelos de IA usando **PyTorch, TensorFlow, etc.** diretamente na GPU da Azure.

Se precisar rodar isso em **Azure Machine Learning (AML)**, pode criar um **Compute Instance** com GPU e usar essa mesma imagem Docker.

Se precisar de mais ajuda, me avise! üöÄ

## **6. Azure Container Instance, Azure Kubernet Service e Azure CLI**
Claro! Vamos abordar de forma clara e pr√°tica os dois servi√ßos citados: **Azure Kubernetes Service (AKS)** e **Azure Container Instances (ACI)**. Ambos s√£o solu√ß√µes para rodar cont√™ineres no Azure, mas com **n√≠veis diferentes de complexidade, controle e escalabilidade**.

### üîπ **6.1. Azure Container Instances (ACI)**

#### ‚úÖ Ideal para:
- Executar **cont√™ineres simples e r√°pidos**, sem se preocupar com infraestrutura.
- Casos de uso **tempor√°rios** ou de **baixa escala**.
- Testes, tarefas autom√°ticas ou pequenas APIs.

#### üß± Requisitos b√°sicos:
- Ter uma **imagem Docker** pronta (no Docker Hub, Azure Container Registry ou outro reposit√≥rio).
- Ter o **Azure CLI** instalado.

#### üöÄ Como come√ßar:

##### 1. Login no Azure:
```bash
az login
```

##### 2. Criar um grupo de recursos:
```bash
az group create --name MeuGrupo --location eastus
```

##### 3. Criar uma inst√¢ncia de cont√™iner rodando uma imagem Docker:
```bash
az container create \
  --resource-group MeuGrupo \
  --name meucontainer \
  --image nginx \
  --cpu 1 \
  --memory 1 \
  --ports 80 \
  --dns-name-label meucontainerdns123 \
  --location eastus
```

##### 4. Verificar se est√° funcionando:
```bash
az container show --resource-group MeuGrupo --name meucontainer --query ipAddress.fqdn
```

Abra o endere√ßo no navegador e veja o NGINX rodando.

#### ‚ö†Ô∏è Limita√ß√µes:
- **N√£o suporta GPU diretamente**.
- N√£o √© adequado para cargas de trabalho complexas ou escal√°veis.

---

### üî∑ **2.2. Azure Kubernetes Service (AKS)**

#### ‚úÖ Ideal para:
- Aplica√ß√µes complexas em **escala**.
- Uso de m√∫ltiplos cont√™ineres, **balanceamento de carga**, **autoescalonamento**, **GPU**, etc.
- Ambientes de produ√ß√£o com **resili√™ncia e alta disponibilidade**.

#### üß± Requisitos b√°sicos:
- Conhecimento b√°sico de **Kubernetes** (pods, deployments, services).
- Azure CLI e a extens√£o AKS:
```bash
az extension add --name aks
```
- Docker instalado (para criar imagens locais).

---

#### üöÄ Como come√ßar com AKS:

##### 1. Criar grupo de recursos:
```bash
az group create --name MeuGrupoAKS --location eastus
```

##### 2. Criar um cluster AKS:
```bash
az aks create \
  --resource-group MeuGrupoAKS \
  --name MeuClusterAKS \
  --node-count 1 \
  --enable-addons monitoring \
  --generate-ssh-keys
```

##### 3. Conectar ao cluster:
```bash
az aks get-credentials --resource-group MeuGrupoAKS --name MeuClusterAKS
```

##### 4. Criar um app simples (por exemplo, NGINX):
```bash
kubectl create deployment nginx --image=nginx
kubectl expose deployment nginx --port=80 --type=LoadBalancer
```

##### 5. Obter o IP p√∫blico:
```bash
kubectl get services
```

Abra o IP no navegador.

---

#### ‚öôÔ∏è GPU no AKS:
Se desejar rodar modelos com GPU (como no Ollama), crie **n√≥s com GPU**:
```bash
az aks nodepool add \
  --resource-group MeuGrupoAKS \
  --cluster-name MeuClusterAKS \
  --name gpupool \
  --node-count 1 \
  --node-vm-size Standard_NC6 \
  --enable-node-public-ip \
  --aks-custom-headers UseGPUDedicatedVHD=true
```

Voc√™ pode ent√£o agendar workloads para esse pool com `nodeSelector`.

---

#### üÜö Comparativo R√°pido

| Recurso | ACI | AKS |
|--------|-----|-----|
| Setup | Muito simples | Mais complexo |
| Escalabilidade | Limitada | Altamente escal√°vel |
| Suporte a GPU | ‚ùå N√£o | ‚úÖ Sim |
| Ideal para | Testes, jobs simples | Produ√ß√£o, cargas complexas |
| Pre√ßo | Paga por segundo de uso | Cobra por VM (mesmo ociosa) |
| Gerenciamento | Zero infra | Voc√™ gerencia o cluster |

---

#### üîö Conclus√£o

- **Comece com o ACI** se quiser testar rapidamente seu cont√™iner.
- **Use o AKS** se estiver construindo uma **infraestrutura robusta para produ√ß√£o**, especialmente com **GPU** para LLMs.

---

Se quiser, posso te fornecer:
- Um **template YAML para AKS** com Ollama rodando.
- Um **workflow CI/CD do GitHub Actions** para publicar imagens no Azure e deploy autom√°tico.

√â s√≥ pedir! üöÄ
